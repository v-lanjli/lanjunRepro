{
	"name": "04 Using delta lake in Synapse",
	"properties": {
		"nbformat": 4,
		"nbformat_minor": 2,
		"bigDataPool": {
			"referenceName": "large",
			"type": "BigDataPoolReference"
		},
		"sessionProperties": {
			"driverMemory": "28g",
			"driverCores": 4,
			"executorMemory": "28g",
			"executorCores": 4,
			"numExecutors": 2
		},
		"metadata": {
			"language_info": {
				"name": "python"
			},
			"a365ComputeOptions": {
				"id": "/subscriptions/051ddeca-1ed6-4d8b-ba6f-1ff561e5f3b3/resourceGroups/bigdataqa/providers/Microsoft.Synapse/workspaces/bigdataqa0407/bigDataPools/large",
				"name": "large",
				"type": "Spark",
				"endpoint": "https://bigdataqa0407.dev.azuresynapse.net/livyApi/versions/2019-11-01-preview/sparkPools/large",
				"auth": {
					"type": "AAD",
					"authResource": "https://dev.azuresynapse.net"
				},
				"sparkVersion": "2.4",
				"nodeCount": 3,
				"cores": 16,
				"memory": 112
			}
		},
		"cells": [
			{
				"cell_type": "markdown",
				"source": [
					"# Using delta lake in Synaspe Spark\n",
					"Synapse is compatible with Linux Foundation Delta Lake. Delta Lake is an open-source storage layer that brings ACID (atomicity, consistency, isolation, and durability) transactions to Apache Spark and big data workloads.\n",
					"\n",
					"This notebook provides examples of how to update, merge and delete delta lake tables in Synapse.\n",
					"\n",
					"## Pre-requisites\n",
					"In this notebook we are going to save your delta table to workspace's primary storage account. You are required to be a **Blob Storage Contributor** in the ADLS Gen2 account (or folder) you will access.\n",
					""
				]
			},
			{
				"cell_type": "markdown",
				"source": [
					"## Load a sample data\n",
					"\n",
					"Let's first load the [public holidays](https://azure.microsoft.com/en-us/services/open-datasets/catalog/public-holidays/) of last 6 months from Azure Open datasets as a sample.\n",
					""
				]
			},
			{
				"cell_type": "code",
				"source": [
					"from azureml.opendatasets import PublicHolidays\n",
					"\n",
					"from datetime import datetime\n",
					"from dateutil import parser\n",
					"from dateutil.relativedelta import relativedelta\n",
					"\n",
					"\n",
					"end_date = datetime.today()\n",
					"start_date = datetime.today() - relativedelta(months=6)\n",
					"hol = PublicHolidays(start_date=start_date, end_date=end_date)\n",
					"hol_df = hol.to_spark_dataframe()"
				],
				"execution_count": 3
			},
			{
				"cell_type": "code",
				"source": [
					"# Display 10 rows\n",
					"hol_df.show(10, truncate = False)"
				],
				"execution_count": 4
			},
			{
				"cell_type": "markdown",
				"source": [
					"## Write data to the delta lake table\n",
					""
				]
			},
			{
				"cell_type": "code",
				"source": [
					"# Set the strorage path info\n",
					"# Primary storage info\n",
					"account_name = 'ltianwestus2gen2' # fill in your primary account name\n",
					"container_name = 'mydefault' # fill in your container name\n",
					"relative_path = 'samplenb/' # fill in your relative folder path\n",
					"\n",
					"adls_path = 'abfss://%s@%s.dfs.core.windows.net/%s' % (container_name, account_name, relative_path)\n",
					"print('Primary storage account path: ' + adls_path)\n",
					"\n",
					"# Delta lake relative path\n",
					"delta_relative_path = adls_path + 'delta/holiday/'\n",
					"print('Delta lake path: ' + delta_relative_path)"
				],
				"execution_count": 5
			},
			{
				"cell_type": "code",
				"source": [
					"# Filter out indian holidays\n",
					"hol_df_IN = hol_df[(hol_df.countryRegionCode == \"IN\")]\n",
					"hol_df_IN.show(5, truncate = False)"
				],
				"execution_count": 6
			},
			{
				"cell_type": "code",
				"source": [
					"#Let's write the data in the delta table. \n",
					"hol_df_IN.write.mode(\"overwrite\").format(\"delta\").partitionBy(\"holidayName\").save(delta_relative_path)"
				],
				"execution_count": 7
			},
			{
				"cell_type": "code",
				"source": [
					"delta_data = spark.read.format(\"delta\").load(delta_relative_path)\n",
					"delta_data.show()"
				],
				"execution_count": 8
			},
			{
				"cell_type": "markdown",
				"source": [
					"## Overwirte the entire delta table\n",
					""
				]
			},
			{
				"cell_type": "code",
				"source": [
					"#Let's overwrite the entire delta file with 1 record\n",
					"\n",
					"hol_df_JP= hol_df[(hol_df.countryRegionCode == \"JP\")]\n",
					"hol_df_JP.write.format(\"delta\").mode(\"overwrite\").save(delta_relative_path)"
				],
				"execution_count": 9
			},
			{
				"cell_type": "code",
				"source": [
					"delta_data = spark.read.format(\"delta\").load(delta_relative_path)\n",
					"delta_data.show()"
				],
				"execution_count": 10
			},
			{
				"cell_type": "markdown",
				"source": [
					"## Merge new data based on given merge condition "
				]
			},
			{
				"cell_type": "code",
				"source": [
					"# Upsert (merge) the United States' holiday data with Japan's\n",
					" \n",
					"from delta.tables import *\n",
					"\n",
					"deltaTable = DeltaTable.forPath(spark,delta_relative_path)\n",
					"\n",
					"hol_df_US= hol_df[(hol_df.countryRegionCode == \"US\")]\n",
					"\n",
					"\n",
					"deltaTable.alias(\"hol_df_JP\").merge(\n",
					"     source = hol_df_US.alias(\"hol_df_US\"),\n",
					"     condition = \"hol_df_JP.countryRegionCode = hol_df_US.countryRegionCode\"\n",
					"    ).whenMatchedUpdate(set = \n",
					"    {}).whenNotMatchedInsert( values = \n",
					"    {\n",
					"        \"countryOrRegion\" : \"hol_df_US.countryOrRegion\",\n",
					"        \"holidayName\" : \"hol_df_US.holidayName\",\n",
					"        \"normalizeHolidayName\" : \"hol_df_US.normalizeHolidayName\",\n",
					"        \"isPaidTimeOff\":\"hol_df_US.isPaidTimeOff\",\n",
					"        \"countryRegionCode\":\"hol_df_US.countryRegionCode\",\n",
					"        \"date\":\"hol_df_US.date\"\n",
					"    }\n",
					"    ).execute()\n",
					"\n",
					"\n",
					"deltaTable.toDF().show()"
				],
				"execution_count": 11
			},
			{
				"cell_type": "markdown",
				"source": [
					"## Update table on the rows that match the given condition\n",
					""
				]
			},
			{
				"cell_type": "code",
				"source": [
					"# Update column the 'null' value in 'isPaidTimeOff' with 'false'\n",
					"\n",
					"from pyspark.sql.functions import *\n",
					"deltaTable.update(\n",
					"    condition = (col(\"isPaidTimeOff\").isNull()),\n",
					"    set = {\"isPaidTimeOff\": \"false\"})\n",
					"\n",
					"deltaTable.toDF().show()"
				],
				"execution_count": 12
			},
			{
				"cell_type": "markdown",
				"source": [
					"## Delte data from the table that match the given condition\n",
					""
				]
			},
			{
				"cell_type": "code",
				"source": [
					"print(\"Row count before delete: \")\n",
					"print(deltaTable.toDF().count())\n",
					"\n",
					"\n",
					"# Delte data with date later than 2020-01-01\n",
					"deltaTable.delete (\"date > '2020-01-01'\")\n",
					"\n",
					"\n",
					"print(\"Row count after delete:  \")\n",
					"print(deltaTable.toDF().count())\n",
					"deltaTable.toDF().show()"
				],
				"execution_count": 13
			},
			{
				"cell_type": "markdown",
				"source": [
					"## Get the operation history of the delta table\n",
					""
				]
			},
			{
				"cell_type": "code",
				"source": [
					"fullHistoryDF = deltaTable.history()\n",
					"lastOperationDF = deltaTable.history(1)\n",
					"\n",
					"print('Full history DF: ')\n",
					"fullHistoryDF.show(truncate = False)\n",
					"\n",
					"print('lastOperationDF: ')\n",
					"lastOperationDF.show(truncate = False)"
				],
				"execution_count": 14
			}
		]
	}
}