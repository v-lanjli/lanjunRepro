{
	"name": "01 Read and write data from Azure Blob Storage WASB",
	"properties": {
		"nbformat": 4,
		"nbformat_minor": 2,
		"bigDataPool": {
			"referenceName": "smalll",
			"type": "BigDataPoolReference"
		},
		"sessionProperties": {
			"driverMemory": "28g",
			"driverCores": 4,
			"executorMemory": "28g",
			"executorCores": 4,
			"numExecutors": 2
		},
		"metadata": {
			"kernelspec": {
				"name": "synapse_pyspark",
				"display_name": "Synapse PySpark"
			},
			"language_info": {
				"name": "python"
			},
			"a365ComputeOptions": {
				"id": "/subscriptions/051ddeca-1ed6-4d8b-ba6f-1ff561e5f3b3/resourceGroups/bigdataqa/providers/Microsoft.Synapse/workspaces/bigdataqa0407/bigDataPools/smalll",
				"name": "smalll",
				"type": "Spark",
				"endpoint": "https://bigdataqa0407.dev.azuresynapse.net/livyApi/versions/2019-11-01-preview/sparkPools/smalll",
				"auth": {
					"type": "AAD",
					"authResource": "https://dev.azuresynapse.net"
				},
				"sparkVersion": "2.4",
				"nodeCount": 3,
				"cores": 4,
				"memory": 28
			}
		},
		"cells": [
			{
				"cell_type": "markdown",
				"source": [
					"# Access data on Azure Storage Blob (WASB) with Synapse Spark\n",
					"\n",
					"You can access data on Azure Storage Blob (WASB) with Synapse Spark via following URL:\n",
					"\n",
					"    wasb[s]://<container_name>@<storage_account_name>.blob.core.windows.net/<path>\n",
					"\n",
					"This notebook provides examples of how to read data from WASB into a Spark context and how to write the output of Spark jobs directly into a WASB location."
				],
				"attachments": null
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"## Load a sample data\n",
					"\n",
					"Let's first load the [public holidays](https://azure.microsoft.com/en-us/services/open-datasets/catalog/public-holidays/) of last 6 months from Azure Open datasets as a sample."
				],
				"attachments": null
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					},
					"collapsed": true
				},
				"source": [
					"test"
				],
				"attachments": null,
				"execution_count": null
			},
			{
				"cell_type": "code",
				"source": [
					"from azureml.opendatasets import PublicHolidays\n",
					"\n",
					"from datetime import datetime\n",
					"from dateutil import parser\n",
					"from dateutil.relativedelta import relativedelta\n",
					"\n",
					"\n",
					"end_date = datetime.today()\n",
					"start_date = datetime.today() - relativedelta(months=6)\n",
					"hol = PublicHolidays(start_date=start_date, end_date=end_date)\n",
					"hol_df = hol.to_spark_dataframe()"
				],
				"attachments": null,
				"execution_count": null
			},
			{
				"cell_type": "code",
				"source": [
					"# Display 5 rows\n",
					"hol_df.show(5, truncate = False)"
				],
				"attachments": null,
				"execution_count": null
			},
			{
				"cell_type": "markdown",
				"source": [
					"## Write data to Azure Storage Blob\n",
					"\n",
					"We are going to write the spark dateframe to your Azure Blob Storage (WASB) path using **shared access signature (sas)**. Go to [Azure Portal](https://portal.azure.com/), open your Azure storage blob, select **shared access signature** in the **settings** and generate your sas token. Please make sure to allow contatiner level read and write permission. Fill in the access info for your Azure storage blob in the cell below. \n",
					""
				],
				"attachments": null
			},
			{
				"cell_type": "code",
				"source": [
					"from pyspark.sql import SparkSession\n",
					"from pyspark.sql.types import *\n",
					"\n",
					"# Azure storage access info\n",
					"blob_account_name = 'samplenbblob' # replace with your blob name\n",
					"blob_container_name = 'data' # replace with your container name\n",
					"blob_relative_path = 'samplenb/' # replace with your relative folder path\n",
					"blob_sas_token = r'?sv=2019-02-02&ss=b&srt=sco&sp=rwdlac&se=2021-03-23T17:05:16Z&st=2020-03-24T09:05:16Z&spr=https,http&sig=drtIrL68s07nPW0Q9WEb5XFL6y5Eb7%2BOpmpxGyAHLaw%3D' # replace with your access key"
				],
				"attachments": null,
				"execution_count": null
			},
			{
				"cell_type": "code",
				"source": [
					"# Allow SPARK to access from Blob remotely\n",
					"wasbs_path = 'wasbs://%s@%s.blob.core.windows.net/%s' % (blob_container_name, blob_account_name, blob_relative_path)\n",
					"spark.conf.set('fs.azure.sas.%s.%s.blob.core.windows.net' % (blob_container_name, blob_account_name), blob_sas_token)\n",
					"print('Remote blob path: ' + wasbs_path)"
				],
				"attachments": null,
				"execution_count": null
			},
			{
				"cell_type": "markdown",
				"source": [
					"### Save a dataframe as Parquet, JSON or CSV\n",
					"If you have a dataframe, you can save it to Parquet or JSON with the .write.parquet(), .write.json() and .write.csv() methods respectively.\n",
					"\n",
					"Dataframes can be saved in any format, regardless of the input format.\n",
					""
				],
				"attachments": null
			},
			{
				"cell_type": "code",
				"source": [
					"parquet_path = wasbs_path + 'holiday.parquet'\n",
					"json_path = wasbs_path + 'holiday.json'\n",
					"csv_path = wasbs_path + 'holiday.csv'\n",
					"print('parquet file path: ' + parquet_path)\n",
					"print('json file pathï¼š ' + json_path)\n",
					"print('csv file path: ' + csv_path)"
				],
				"attachments": null,
				"execution_count": null
			},
			{
				"cell_type": "code",
				"source": [
					"hol_df.write.parquet(parquet_path, mode = 'overwrite')\n",
					"hol_df.write.json(json_path, mode = 'overwrite')\n",
					"hol_df.write.csv(csv_path, mode = 'overwrite', header = 'true')"
				],
				"attachments": null,
				"execution_count": null
			},
			{
				"cell_type": "markdown",
				"source": [
					"### Save a dataframe as text files\n",
					"If you have a dataframe that you want ot save as text file, you must first covert it to an RDD and then save that RDD as a text file.\n",
					""
				],
				"attachments": null
			},
			{
				"cell_type": "code",
				"source": [
					"# Define the text file path\n",
					"text_path = wasbs_path + 'holiday.txt'\n",
					"print('text file path: ' + text_path)"
				],
				"attachments": null,
				"execution_count": null
			},
			{
				"cell_type": "code",
				"source": [
					"# Covert spark dataframe into RDD \n",
					"hol_RDD = hol_df.rdd\n",
					"type(hol_RDD)"
				],
				"attachments": null,
				"execution_count": null
			},
			{
				"cell_type": "markdown",
				"source": [
					"If you have an RDD, you can convert it to a text file like the following:\n",
					""
				],
				"attachments": null
			},
			{
				"cell_type": "code",
				"source": [
					" # Save RDD as text file\n",
					"hol_RDD.saveAsTextFile(text_path)"
				],
				"attachments": null,
				"execution_count": null
			},
			{
				"cell_type": "markdown",
				"source": [
					"# Read data from Azure Storage Blob\n",
					""
				],
				"attachments": null
			},
			{
				"cell_type": "markdown",
				"source": [
					"### Create a dataframe from parquet files\n",
					""
				],
				"attachments": null
			},
			{
				"cell_type": "code",
				"source": [
					"df_parquet = spark.read.parquet(parquet_path)"
				],
				"attachments": null,
				"execution_count": null
			},
			{
				"cell_type": "markdown",
				"source": [
					"### Create a dataframe from JSON files\n",
					""
				],
				"attachments": null
			},
			{
				"cell_type": "code",
				"source": [
					"df_json = spark.read.json(json_path)"
				],
				"attachments": null,
				"execution_count": null
			},
			{
				"cell_type": "markdown",
				"source": [
					"### Create a dataframe from CSV files\n",
					""
				],
				"attachments": null
			},
			{
				"cell_type": "code",
				"source": [
					"df_csv = spark.read.csv(csv_path, header = 'true')"
				],
				"attachments": null,
				"execution_count": null
			},
			{
				"cell_type": "markdown",
				"source": [
					"### Create an RDD from text file\n",
					""
				],
				"attachments": null
			},
			{
				"cell_type": "code",
				"source": [
					"text = sc.textFile(text_path)"
				],
				"attachments": null,
				"execution_count": null
			}
		]
	}
}